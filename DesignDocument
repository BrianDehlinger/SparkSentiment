

Rapid Prototype is first step: 

Documentation for Rapid Prototype. On Linux Ubuntu Bioinic(18.04.1 LTS)

Run these commands in linux. Omit sudo if you are root

sudo snap install docker 
sudo docker pull nshou/elasticsearch-kibana
sudo docker run -d -p 9200:9200 -p 5601:5601 nshou/elasticsearch-kibana

These commands will result in docker being installed and a docker image running on your machine at ports 9200 and 5601 and:

localhost:5601 bringing you to an instance of kibana
localhost:9200 brings you to an instance of Elasticsearch. Awesome!


Make a config.py file with the two consumer keys and the access token and access secret from your application.
This is necessary for the twitter API to work.

Geolocation data can be used as well.



Requirements:

What Data?
Historical and current(streaming data)- Sentiment about U.S. National Parks from Twitter.

Granularity of Data?
Data should be searchable by location. Here are some examples of what we want to be able to do:

Learn how people in Illinois feel about all the national parks in the text file.
Learn how people in Illinois feel about a particular national park or set of national parks.
Learn how people in Illinois feel about the national parks in a particular geographic location.
Learn how people in a specific geographic region as determined by radius, latititude and longitude or state or group of states feel about the above.


How should sentiment be communicated?

A visualization utilizing shades of red/blue where deeper red indicates stronger dislike and deeper blue indicates stronger like on the United States map.
Particlular national parks will be pinned on a side map of the United States.



Inputs to the system:

1) Data from Streaming Twitter API(such as Tweepy) given a query of a national park or set of national parks.
  a) Source: Twitter API/Tweepy
  b) Accuracy: Data must be filtered based on location or must be universal. 
  c) Range of values: Text data of max(length requirements in Twitter).
  d) Frequency: Whenver a new tweet is generated, one query or set of queries for historical data only.
  e) Output: ElasticSearch or Postgresql database.
    a)

2) Sentiment values generated by Machine Learning Framework
  a) Source: Machine Learning API for sentiment analysis such as TextBlob
  b) Accuracy: Confidience Interval or Similar could be calculated and will likely be added after core.
  c) Range of values: Polarity scale. 





Possible changes/additions:

Future data from another source such as Reddit to add data for universal sentiment rather than geographic.
More NLTK additions such as determining what keywords are associated with dislike and what keywords are associated with like.
Summary statistics. 







